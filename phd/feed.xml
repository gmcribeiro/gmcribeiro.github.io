<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://goncalor00.github.io/phd/feed.xml" rel="self" type="application/atom+xml"/><link href="https://goncalor00.github.io/phd/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-09T23:11:12+00:00</updated><id>https://goncalor00.github.io/phd/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Simulating 2D motion magnification</title><link href="https://goncalor00.github.io/phd/reports/2025/simulating_motion_magnification/" rel="alternate" type="text/html" title="Simulating 2D motion magnification"/><published>2025-02-14T16:40:16+00:00</published><updated>2025-02-14T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2025/simulating_motion_magnification</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2025/simulating_motion_magnification/"><![CDATA[<h2 id="velocity-magnified">Velocity magnified</h2> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/vel_ground_truth_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Magnification using the known pixel shifts from the original video and magnifying the velocity of the pixels.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/vel_synthetic_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Magnification applied directly to the original video without knowing the ground truth of the pixel shifts using a simplified version of phase-based motion magnification.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/vel_phase_magnified_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <h2 id="shift-magnified-problem">Shift magnified problem</h2> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/shift_prob_ground_truth_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/shift_prob_synthetic_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/shift_prob_phase_magnified_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <h2 id="shift-magnified-problem-with-velocity-magnification">Shift magnified problem with velocity magnification</h2> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/shift_vel_ground_truth_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/shift_vel_synthetic_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/shift_vel_phase_magnified_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <h2 id="higher-resolution">Higher resolution</h2> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/hd_ground_truth_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/hd_synthetic_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/02_2025/hd_phase_magnified_motion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div>]]></content><author><name></name></author><category term="meeting-support"/><category term="Motion magnification"/><category term="Simulation"/><summary type="html"><![CDATA[Meeting support]]></summary></entry><entry><title type="html">Review articles for MIE</title><link href="https://goncalor00.github.io/phd/reports/2025/review_article_mie/" rel="alternate" type="text/html" title="Review articles for MIE"/><published>2025-01-19T16:40:16+00:00</published><updated>2025-01-19T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2025/review_article_mie</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2025/review_article_mie/"><![CDATA[<p>Preliminary paper:</p> <style>.pdf-embed-wrap-6a3fdc9e-41bd-4879-b160-567d059f521b{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-6a3fdc9e-41bd-4879-b160-567d059f521b{height:100%}.pdf-link-6a3fdc9e-41bd-4879-b160-567d059f521b{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-6a3fdc9e-41bd-4879-b160-567d059f521b iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-6a3fdc9e-41bd-4879-b160-567d059f521b"> <div class="pdf-link-6a3fdc9e-41bd-4879-b160-567d059f521b"> <a href="/phd/assets/posts/pdf/01_2025/MIE_Review_paper_draft.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-6a3fdc9e-41bd-4879-b160-567d059f521b"> <iframe src="/phd/assets/posts/pdf/01_2025/MIE_Review_paper_draft.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div> <p><br/></p> <p>Final paper:</p> <style>.pdf-embed-wrap-3e948417-ec2c-4785-905d-a134ef8b1ebd{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-3e948417-ec2c-4785-905d-a134ef8b1ebd{height:100%}.pdf-link-3e948417-ec2c-4785-905d-a134ef8b1ebd{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-3e948417-ec2c-4785-905d-a134ef8b1ebd iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-3e948417-ec2c-4785-905d-a134ef8b1ebd"> <div class="pdf-link-3e948417-ec2c-4785-905d-a134ef8b1ebd"> <a href="/phd/assets/posts/pdf/01_2025/93193_mie_review_paper.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-3e948417-ec2c-4785-905d-a134ef8b1ebd"> <iframe src="/phd/assets/posts/pdf/01_2025/93193_mie_review_paper.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div> <p><br/></p>]]></content><author><name></name></author><category term="meeting-support"/><category term="Motion magnification"/><category term="3D Motion magnification"/><category term="Research"/><summary type="html"><![CDATA[Meeting support]]></summary></entry><entry><title type="html">Ideas for the research paper</title><link href="https://goncalor00.github.io/phd/reports/2024/ideas_review_paper/" rel="alternate" type="text/html" title="Ideas for the research paper"/><published>2024-12-06T16:40:16+00:00</published><updated>2024-12-06T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/ideas_review_paper</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/ideas_review_paper/"><![CDATA[<p><a href="https://3d-motion-magnification.github.io/">3D Motion Magnification: Visualizing Subtle Motions with Time-Varying Neural Fields</a></p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/11_2024/fork_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>This approach is based on <a href="https://www.matthewtancik.com/nerf">NeRF</a> and already supports the use of a single-camera. However there is room for improvement:</p> <ul> <li>NeRF requires lots of viewpoints.</li> <li>NeRF is computationally heavy - I can only use pre-trained weights on the author’s examples since the laptop doesn’t have enough graphical memory. The compute time is around 30 seconds per frame, even at the inference stage.</li> <li>NeRF needs the camera pose and orientation for each image in the training stage. Some authors use the <a href="https://colmap.github.io/">COLMAP</a> tool to retrieve this information from the images. (COLMAP works similarly to the method I used to retrieve the camera’s pose). The authors of <a href="https://3d-motion-magnification.github.io/">3D Motion Magnification: Visualizing Subtle Motions with Time-Varying Neural Fields</a> claim that, for real-world environments, the accuracy of these methods may not be enough for motion magnification since “inaccurate pose estimation would exacerbate the ambiguity between camera motion and scene motion, which could hinder magnifying subtle scene motions or lead to false motion magnification. Therefore, real-world data should be captured under conditions where accurate camera intrinsic and extrinsic parameters are accessible, either from reliable RGB-based SfM with textured surfaces in the scene, or from cameras that support 6-DoF tracking during capture”. Since MotionScope does the motion magnification in 2D, the methodology would be different by avoiding the motion magnification on the fly, which may allow less accuracy.</li> </ul> <p>Other approaches use multiple cameras to get the 3D displacement in a few points, not the entire image. In addition to that, they require prior camera calibration.</p> <p>Possible review paper title: The use of multi-view systems on phase-based motion magnification</p> <p>Possible paper structure:</p> <ul> <li>Introduction</li> <li>Phase-based motion magnification</li> <li>Multi-view systems</li> <li>Integration of Multi-view Systems and Phase-Based Motion Magnification</li> <li>Tests and results</li> <li>Conclusion</li> </ul> <p>Main takeaways from the deliverables of the immersive week:</p> <p>Research Questions:</p> <ul> <li>Is it more effective to combine multiple viewpoints to compute 3D motion magnification directly, or to first compute 2D motion magnification for each viewpoint and then combine them for 3D motion magnification?</li> <li>Does the latter approach reduce the need for highly accurate calibration?</li> <li>Can a single moving camera, synchronized across viewpoints, replace the use of multiple cameras for 3D motion magnification?</li> <li>Is it possible to perform 3D reconstruction using only a small number of viewpoints and without pre-calibrating the cameras?</li> <li>Is there a more effective way to present the results of 3D motion magnification?</li> </ul> <p>Contributions:</p> <ul> <li>Improved Methodology for 3D Motion Magnification: By reducing calibration requirements, making the process more practical and accessible, and by using a single moving camera instead of multiple cameras, which could lower costs and simplify experimental setups.</li> <li>Efficient 3D Reconstruction with Minimal Viewpoints: By developing methods for accurate 3D reconstruction using fewer viewpoints, reducing hardware requirements and computational complexity, and exploring the potential to bypass pre-calibration, which could streamline the workflow and make the technology more user-friendly.</li> <li>Enhanced Visualization of Results: By proposing new ways to display 3D motion magnification results, such as interactive 3D meshes or point clouds, which could offer better insights and flexibility compared to traditional video-based outputs.</li> </ul>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Motion magnification"/><category term="3D Motion magnification"/><category term="Calibration"/><category term="Research"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">First research on 3D motion magnification</title><link href="https://goncalor00.github.io/phd/reports/2024/first_research_3d_motion_magnification/" rel="alternate" type="text/html" title="First research on 3D motion magnification"/><published>2024-11-25T16:40:16+00:00</published><updated>2024-11-25T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/first_research_3d_motion_magnification</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/first_research_3d_motion_magnification/"><![CDATA[<p>Works on 3D motion magnification</p> <p><a href="https://3d-motion-magnification.github.io/">3D Motion Magnification: Visualizing Subtle Motions with Time-Varying Neural Fields</a></p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/11_2024/fork_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>This approach is based on <a href="https://www.matthewtancik.com/nerf">NeRF</a> and already supports the use of a single-camera</p> <p>Research gaps:</p> <ul> <li>NeRF requires lots of viewpoints.</li> <li>NeRF is computationally heavy - I can only use pre-trained weights on the author’s examples since the laptop doesn’t have enough graphical memory. The compute time is around 30 seconds per frame, even at the inference stage.</li> <li>NeRF needs the camera pose and orientation for each image in the training stage. Some authors use the <a href="https://colmap.github.io/">COLMAP</a> tool to retrieve this information from the images. (COLMAP works similarly to the method I used to retrieve the camera’s pose). The authors of <a href="https://3d-motion-magnification.github.io/">3D Motion Magnification: Visualizing Subtle Motions with Time-Varying Neural Fields</a> claim that, for real-world environments, the accuracy of these methods may not be enough for motion magnification since “inaccurate pose estimation would exacerbate the ambiguity between camera motion and scene motion, which could hinder magnifying subtle scene motions or lead to false motion magnification. Therefore, real-world data should be captured under conditions where accurate camera intrinsic and extrinsic parameters are accessible, either from reliable RGB-based SfM with textured surfaces in the scene, or from cameras that support 6-DoF tracking during capture”. Since MotionScope does the motion magnification in 2D, the methodology would be different by avoiding the motion magnification on the fly, which may allow less accuracy.</li> </ul> <p>The following methods don´t have a 3D graphical representation of the motion magnification. They present quantitative displacement results on convenient points (where it is possible to match the points between two viewpoints with reliability). All of them require camera calibration.</p> <p><a href="https://doi.org/10.1016/j.jsv.2017.06.003">Feasibility of extracting operating shapes using phase-based motion magnification technique and stereo-photogrammetry</a></p> <ul> <li>Improvement of the data quality</li> <li>3D information</li> </ul> <p><a href="https://doi.org/10.1016/j.ymssp.2018.02.006">3D mode shapes characterisation using phase-based motion magnification in large structures using stereoscopic DIC</a></p> <ul> <li>Reduction of noise, allowing the capture of low-amplitude displacements</li> </ul> <p><a href="https://doi.org/10.1016/j.jsv.2022.117244">Target-free 3D tiny structural vibration measurement based on deep learning and motion magnification</a></p> <ul> <li>This method uses the Super Glue feature match algorithm to find the matches and then triangulate to get the 3D displacement.</li> </ul> <p>Research gaps:</p> <ul> <li>Uncalibrated cameras</li> <li>Interactive results presentation - The idea is to generate a mesh/point cloud that we can observe from any perspective</li> </ul> <p>Possible review paper title: The use of multi-view systems on phase-based motion magnification</p> <p>Deliverables for the immersive week:</p> <style>.pdf-embed-wrap-f1ebaf9f-b604-43f1-861b-645904c0bd86{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-f1ebaf9f-b604-43f1-861b-645904c0bd86{height:100%}.pdf-link-f1ebaf9f-b604-43f1-861b-645904c0bd86{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-f1ebaf9f-b604-43f1-861b-645904c0bd86 iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-f1ebaf9f-b604-43f1-861b-645904c0bd86"> <div class="pdf-link-f1ebaf9f-b604-43f1-861b-645904c0bd86"> <a href="/phd/assets/posts/pdf/11_2024/1_imersive_week.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-f1ebaf9f-b604-43f1-861b-645904c0bd86"> <iframe src="/phd/assets/posts/pdf/11_2024/1_imersive_week.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div> <p><br/></p> <p>Research Questions:</p> <ul> <li>Is it more effective to combine multiple viewpoints to compute 3D motion magnification directly, or to first compute 2D motion magnification for each viewpoint and then combine them for 3D motion magnification?</li> <li>Does the latter approach reduce the need for highly accurate calibration?</li> <li>Can a single moving camera, synchronized across viewpoints, replace the use of multiple cameras for 3D motion magnification?</li> <li>Is it possible to perform 3D reconstruction using only a small number of viewpoints and without pre-calibrating the cameras?</li> <li>Is there a more effective way to present the results of 3D motion magnification?</li> </ul> <p>Contributions:</p> <ul> <li>Improved Methodology for 3D Motion Magnification: By reducing calibration requirements, making the process more practical and accessible, and by using a single moving camera instead of multiple cameras, which could lower costs and simplify experimental setups.</li> <li>Efficient 3D Reconstruction with Minimal Viewpoints: By developing methods for accurate 3D reconstruction using fewer viewpoints, reducing hardware requirements and computational complexity, and exploring the potential to bypass pre-calibration, which could streamline the workflow and make the technology more user-friendly.</li> <li>Enhanced Visualization of Results: By proposing new ways to display 3D motion magnification results, such as interactive 3D meshes or point clouds, which could offer better insights and flexibility compared to traditional video-based outputs.</li> </ul> <p>Para a sincronização podiam ser geradas mais imagens para completar o ciclo de vibração e garantir que os pontos estão no mesmo sitio em todos os pontos de vista, ajudando também na reconstrução 3D continua - combate o problema da subamostragem;</p> <p>Perceber o trabalho por de trás do motion scope para perceber o que é que pode estar a faltar e replicar a metodologia do motion scope para conseguir sincronizar pontos de vista; Se não resolver tentar geração interpolada de imagens</p> <p>Uma ideia - uma rede que crie um frame entre dois seguidos e depois chamar iterativamente</p> <p>recusrive generative network</p> <p>O que dar a rede?</p> <p>Interpolation of sub-sampling (…) - começar por sinais e dps ir para coisas mais complexa</p> <p>Tentar arranjar um simulador de vibrações para conseguir arranjar um ground truth - esta pode ser a primeira contribuição ao dar para sintetizar imagens</p> <p><strong>Suma: Simulador; interpolação de imagens; 3D motion magnification</strong></p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Motion magnification"/><category term="3D Motion magnification"/><category term="Research"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">LightGlue on 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/3d_lightglue/" rel="alternate" type="text/html" title="LightGlue on 3D reconstruction"/><published>2024-10-25T16:40:16+00:00</published><updated>2024-10-25T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/3d_lightglue</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/3d_lightglue/"><![CDATA[<p>The current “final” goal can be divided into two parts:</p> <p>“<span style="color:DodgerBlue">Identification techniques of operational vibration modes with MotionScope</span> <span style="color:MediumSeaGreen; font-weight:bold;">assisted by stereoscopy</span>”.</p> <p>From the beginning, I’ve been focusing on the “<span style="color:MediumSeaGreen; font-weight:bold;">assisted by stereoscopy</span>” to create a 3D representation from 2D images. The 3D representation is essential for easier visualization of the magnified motion of the objects.</p> <p>Initial question: <span style="color:MediumSeaGreen; font-weight:bold;">How can we create a 3D representation from 2D images?</span></p> <p>1 - Stereo-based techniques (including multi-view)</p> <ul> <li>Accurately calibrated cameras</li> <li>Dense feature matching across images captured from slightly different viewing angles</li> <li>Triangulation to recover the 3D coordinates of the image pixels.</li> </ul> <p>2- Structure from Motion</p> <ul> <li>This recovers 3D structure by analyzing the motion of the camera and the changes in the images - The only way that I see that this could work is as an auxiliary solution to get more information about the object in the study and about the new camera position and parameters while moving from viewpoint A to viewpoint B.</li> </ul> <p>3 - Shape-from-Silhouette</p> <ul> <li>It reconstructs the 3D shape using the silhouettes of objects from multiple views. This solution was first published in 1994 and is generally used to generate rough, undetailed 3D models quickly. I could try it to see how good the results are and check if this technique is helpful for this application, as it is also used in the loss function of some deep-learning models</li> </ul> <p>4 - Photometric Stereo</p> <ul> <li>Uses variations in shading to infer the 3D shape of objects - I don’t think this is a good path to follow since motion scope relies on the quality of the light source, conflicting with this technique.</li> </ul> <p>5 - Learning-based</p> <ul> <li>There are multiple approaches with learning-based techniques, such as: <ul> <li>3D reconstruction problem as a recognition problem</li> <li>Multi-tasked neural networks - This allows for better generalization of feature recognition</li> <li>End-to-end 3D reconstruction - eliminating the need to design multiple handcrafted stages</li> <li>Single and multi-image 3D reconstruction</li> <li>Depth estimation</li> <li>Disparity maps</li> <li>Feature matching</li> </ul> </li> <li>Some don’t require intrinsic or extrinsic parameters</li> </ul> <p><span style="color:MediumSeaGreen; font-weight:bold;">How can we evaluate 3D representations and compare different methods?</span></p> <p><span style="color:MediumSeaGreen; font-weight:bold;">Are there good enough solutions?</span></p> <p><span style="color:MediumSeaGreen; font-weight:bold;">Can I combine some techniques to improve the results for our use case?</span></p> <p>Until now, what I have done is a 3D reconstruction using uncalibrated stereo, which is similar to <strong>Stereo-based techniques</strong> but with an additional step of “calibration” in the beginning:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/Untitled-Frame-3.svg" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/Untitled-Frame-3.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>In the previous meeting, I showed a promising algorithm for feature-matching, the LightGlue. My idea was to use this algorithm (or something similar) for the “Camera calibration”.</p> <p>A small change in viewpoint: Results with SIFT:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_easy_matching-480.webp 480w,/phd/assets/posts/img/10_2024/sift_easy_matching-800.webp 800w,/phd/assets/posts/img/10_2024/sift_easy_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_easy_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot-480.webp 480w,/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot-800.webp 800w,/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Results with LightGlue:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_easy_matching-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_easy_matching-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_easy_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_easy_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_easy-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_easy-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_easy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_easy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>A more significant change in viewpoint:</p> <p>Results with SIFT:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_hard_matching-480.webp 480w,/phd/assets/posts/img/10_2024/sift_hard_matching-800.webp 800w,/phd/assets/posts/img/10_2024/sift_hard_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_hard_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_hard-480.webp 480w,/phd/assets/posts/img/10_2024/sift_hard-800.webp 800w,/phd/assets/posts/img/10_2024/sift_hard-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_hard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Results with LightGlue:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hard_matching-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hard_matching-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hard_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hard_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hard-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hard-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hard-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Finally, with a big viewpoint change:</p> <p>SIFT:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_hardest_matching-480.webp 480w,/phd/assets/posts/img/10_2024/sift_hardest_matching-800.webp 800w,/phd/assets/posts/img/10_2024/sift_hardest_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_hardest_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>There are not enough matching points to compute camera poses and 3D reconstruction</p> <p>LightGlue:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hardest_matching-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hardest_matching-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hardest_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hardest_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Something went wrong with the camera poses with lightglue, but I haven’t figured out the issue yet.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hardest-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hardest-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hardest-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hardest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Since this new kind of feature-matching algorithm can find so many matches, I wonder if I could use this matching for the “dense matching” part… It would need more viewpoints but would be much lighter computationally. I could also use the epipolar lines to validate the results of the feature-matching…</p> <p>Although there seem to be many feature matches, there are very few for a 3D representation. Dense matching is still needed.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot-480.webp 480w,/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot-800.webp 800w,/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Métricas</p> <p>Criar outliers artificiais para robustez</p> <p>automatizar a definição da dificuldade</p> <p>Gazebo</p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Testing LightGlue</title><link href="https://goncalor00.github.io/phd/reports/2024/testing_lightglue/" rel="alternate" type="text/html" title="Testing LightGlue"/><published>2024-10-11T16:40:16+00:00</published><updated>2024-10-11T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/testing_lightglue</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/testing_lightglue/"><![CDATA[<p>In the last meeting, we concluded that I should research more state-of-the-art solutions that solve entirely or partially the problems of camera pose retrieval and 3D reconstruction. Another idea has been to explore ways to use learning to find the homography, camera position, etc. It turns out that it is all related, and it is what people have been doing.</p> <p>So far, I have only explored feature matching, and, to be honest, I don’t have the knowledge (yet) of machine learning to understand how these algorithms were made. I started a machine learning course, and I did 1/4 of the course. I paused that course because I realized that my knowledge of probabilities wasn’t near enough to understand the machine learning concepts. So, I’m now doing a course about probabilities for machine learning.</p> <p>I tried one detector-based algorithm - LightGlue - for feature match on very challenging scenarios (drastic viewpoint changes), but the results speak for themselves:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/LightGlue_feature_match-480.webp 480w,/phd/assets/posts/img/10_2024/LightGlue_feature_match-800.webp 800w,/phd/assets/posts/img/10_2024/LightGlue_feature_match-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/LightGlue_feature_match.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/LightGlue_feature_match_2-480.webp 480w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_2-800.webp 800w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/LightGlue_feature_match_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/LightGlue_feature_match_3-480.webp 480w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_3-800.webp 800w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/LightGlue_feature_match_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>There are still some errors, but this is a massive improvement compared to what I used.</p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Paper sheet holder</title><link href="https://goncalor00.github.io/phd/reports/2024/paper_sheet_holder/" rel="alternate" type="text/html" title="Paper sheet holder"/><published>2024-07-16T16:40:16+00:00</published><updated>2024-07-16T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/paper_sheet_holder</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/paper_sheet_holder/"><![CDATA[<p>During this time, I didn’t advance much in the work itself, as I spent the time learning more about the basics of computer vision. Meanwhile, I built a support for the sheet paper:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/07_2024/1000010432-480.webp 480w,/phd/assets/posts/img/07_2024/1000010432-800.webp 800w,/phd/assets/posts/img/07_2024/1000010432-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/07_2024/1000010432.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/07_2024/1000010431-480.webp 480w,/phd/assets/posts/img/07_2024/1000010431-800.webp 800w,/phd/assets/posts/img/07_2024/1000010431-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/07_2024/1000010431.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/07_2024/1000010430-480.webp 480w,/phd/assets/posts/img/07_2024/1000010430-800.webp 800w,/phd/assets/posts/img/07_2024/1000010430-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/07_2024/1000010430.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="meeting-support"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting support]]></summary></entry><entry><title type="html">Improving 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/improving_3d_report/" rel="alternate" type="text/html" title="Improving 3D reconstruction"/><published>2024-06-21T17:40:16+00:00</published><updated>2024-06-21T17:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/improving_3d_report</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/improving_3d_report/"><![CDATA[<h1 id="meeting-report">Meeting report</h1> <p>In this meeting, I showed my ideas for improving 3D reconstruction.</p> <h1 id="next-meeting-16072024-">Next meeting (16/07/2024 :??)</h1> <p>Explore solutions from other people and explore more papers</p> <p>Start exploring neural networks</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting report]]></summary></entry><entry><title type="html">Improving 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/improving_3d/" rel="alternate" type="text/html" title="Improving 3D reconstruction"/><published>2024-06-21T16:40:16+00:00</published><updated>2024-06-21T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/improving_3d</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/improving_3d/"><![CDATA[<p>The idea of the previous meeting was to use more viewpoints, but the current “solution” is not robust enough. So, I worked more on studying ways to improve the robustness of each pipeline step.</p> <h3 id="quick-concept-recap">Quick concept recap</h3> <p>Knowing the fundamental matrix (obtained by knowing at least 8 matching points between 2 images), we can map a point of the first image to a line of the second image. The correspondent point, if present in the image, will be somewhere in that line.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2314-59-48-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="caption"> This narrows the search to a 1D search, simplifying the problem considerably. Doing this to the entire image allows, with the intrinsic parameters and more processing, the creation of a 3D cloud. This process is called (not entirely sure) dense matching. </div> <p>I started by trying to improve the “dense matching” algorithm since the matching algorithm in the 1D search is very simplistic and not robust enough. So, I started adapting the code to be compatible with feature-detection algorithms, which I believe will give much better results but take much more time to compute.</p> <p>This leads to two options:</p> <ul> <li> Send the ROI of the image for the 1D search: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1)-480.webp 480w,/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1)-800.webp 800w,/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1).jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> I could not figure out if the feature-detection algorithms ignore the black region or not, and that's the reason why there is an option 2 </li> <li> The second option is more tricky. It involves sending a rectangular image to the feature-detection algorithm to ensure that only the ROI is analyzed. To achieve that, the first step is to rotate the image with the angle of inclination of the epipolar line resulting on the image of the window “Rotated image”; This way, the ROI is a rectangular section of the image, like in the window “Cropped to the window height”, removing the black regions. <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy)-480.webp 480w,/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy)-800.webp 800w,/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> I have some doubts about this method, starting with the fact that the epipolar line is not well defined (the thickness of the line is not 1 pixel—window “Section of the cropped image”). Worse than that, I don’t know if applying the feature-detection algorithm to the entire image with a very high number of keypoints and then applying the ROI only to the feature-match algorithm won't give similar results with less computational cost. </li> </ul> <p>Eventually, I put these studies on hold and studied ways to find the common region between the two viewpoints. This will help in the future to solve the occlusions problem. ChatGPT and Bing chat gave me the following directions:</p> <ul> <li> “Traditional” approaches: <ul> <li> Estimate homography using feature detection and matching algorithms and use the homography to warp one image onto the other and find common regions. Here is an illustration of warping operation:<br/> Image 1: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/img_0_1-480.webp 480w,/phd/assets/posts/img/06_2024/img_0_1-800.webp 800w,/phd/assets/posts/img/06_2024/img_0_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/img_0_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> Image 2: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/img_0_2-480.webp 480w,/phd/assets/posts/img/06_2024/img_0_2-800.webp 800w,/phd/assets/posts/img/06_2024/img_0_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/img_0_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> Aligned image 1: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024-480.webp 480w,/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024-800.webp 800w,/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> The chat only suggests doing a bit-wise operation to find the common region, but that doesn’t make sense since the background is totally different. I kept this here because the object (a sheet of paper) gets a similar shape between the two images, and I could use that to identify and isolate the object. </li> <li> Block Comparison: Basically, divide the image into small blocks and compare each block of image 1 to every block of image 2 and save the blocks with similarity bigger than a threshold </li> </ul> </li> <li> Machine learning: From the two chats, the suggestions were: Siamese networks, collaborative segmentation, stereo matching networks, image registration networks, and deep image matching. I didn’t explore these fields because I felt that I needed a deeper understanding of machine learning, and I started taking an online course on the subject. </li> </ul> <p>During these studies, I had a problem with my computer: I neglected the updates and started not being able to install anything. I formatted the computer and took the opportunity to start working only on docker environments to prevent losing the dependencies of the code that I have in the event of having to format the computer again. When I tested the code, it was much slower in the C++ part. After building from source different versions of OpenCV, GCC an G++ I finally found the issue 🤦‍♂️:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot-480.webp 480w,/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot-800.webp 800w,/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot.png" class="img-fluid rounded z-depth-1" width="250" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">First steps on 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/calibration_3d/" rel="alternate" type="text/html" title="First steps on 3D reconstruction"/><published>2024-05-24T16:40:16+00:00</published><updated>2024-05-24T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/calibration_3d</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/calibration_3d/"><![CDATA[<p>I started by trying to build an entire pipeline to see the end results of the 3D points. This way it is easier to see if something is going wrong in one of the steps.</p> <p>I tried to retrieve the intrinsic parameters of my smartphone’s camera from the image’s metadata, but I was getting weird results. Then I tried to get them manually, but someone in the lab reminded me that smartphones have auto-focus and the parameters change. To avoid messing with my phone, I simply used another camera that was available and not being used by anyone in the lab (an Orbbec Astra Pro) and got the intrinsic parameters using a chessboard and a ROS tool</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240424_162653.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I used SIFT once more to get the matching points from two viewpoints:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Linked%20Points-480.webp 480w,/phd/assets/posts/img/05_2024/Linked%20Points-800.webp 800w,/phd/assets/posts/img/05_2024/Linked%20Points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Linked%20Points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Then, found the Essential matrix → Rotation and Translation → Projection matrices</p> <p>With the matching points and projection matrices, I triangulated and plotted the points (ignoring the distortion parameters) and got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/3D%20plots.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>But the Essential matrix is defined only up to scale (in the illustration it is the fundamental matrix, but the essential matrix comes from the fundamental matrix knowing the intrinsic parameters).</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Source: <a href="https://www.youtube.com/watch?v=izpYAwJ0Hlw&amp;list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo&amp;index=10">https://www.youtube.com/watch?v=izpYAwJ0Hlw&amp;list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo&amp;index=10</a> </div> <p>OpenCV has a function that solves this by setting the distance between cameras to 1, which means that the distance between points is relative to the distance between cameras. We can retrieve the real scale factor by having an object in the image of known dimensions.</p> <p>Using the distortion factors, I got weird results and I haven’t found out yet what I did wrong:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/3DPlotdistort.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Re-calibrating the camera with more images and still ignoring the distortion parameters, the results are apparently better</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/teste.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Then I experimented with bending the paper like this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/img0_perpendicular-480.webp 480w,/phd/assets/posts/img/05_2024/img0_perpendicular-800.webp 800w,/phd/assets/posts/img/05_2024/img0_perpendicular-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/img0_perpendicular.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>With SIFT I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/linked_pointsv2-480.webp 480w,/phd/assets/posts/img/05_2024/linked_pointsv2-800.webp 800w,/phd/assets/posts/img/05_2024/linked_pointsv2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/linked_pointsv2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>And plotting, I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/bend.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>This result gave me more confidence about the reliability of the previous steps.</p> <p>Then, I started working on a 3D reconstruction of the object. I started by trying some OpenCV functions. The functions that I found create disparity maps, and the goal was to create a 3D plot from those maps. With this approach, I only got bad results without any meaning, and I gave up.</p> <p>Using the two images from the two viewpoints and doing the dot product between the fundamental matrix and the coordinates of the pixel in analyses in the first image, the result is a vector containing the $a$, $b$, and $c$ parameters of the epipolar line ($ax + by + c = 0$) in the second image:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2215-44-12-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Inside the epipolar line will be the point of image 2 that matches the point in analyses of image 1. This reduces the search of the matching point by a lot, being a 1D search instead of a 2D search.</p> <p>Although the search is less than without the epipolar line, there are many points, and even using the simplest algorithm possible to compare the windows of pixels (absolute distance between arrays—cv2.norm()) from the two images, the time of execution using Python is more than 10 minutes.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2314-59-48-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>The cv2.norm() is not ideal for this use case and generates a lot of errors, but by removing isolated points, I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/point_cloud.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>After that, I decided to try to optimize the code, but I only got minor improvements. So, I took a C++ course and implemented the slowest function in C++. The major part of the program is written in Python, and the function with high computation cost is written in C++ and compiled. With this, the computation time of the function is less than 30 seconds, more than 30x faster than Python.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240522_151420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240523_191350.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The current pipeline looks like this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/pointcloud_explained.svg" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/pointcloud_explained.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Unanswered questions:</p> <ul> <li>What are the best algorithms (green boxes) for the current pipeline?</li> <li>How can I evaluate the quality of the results?</li> <li>How can I estimate the intrinsic parameters after changing the focal length?</li> <li>In a stream of images (after motion scope), should I rerun the current pipeline each time, or should I use some kind of tracking algorithm?</li> </ul>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><category term="Calibration"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry></feed>