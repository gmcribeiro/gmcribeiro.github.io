<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://goncalor00.github.io/phd/feed.xml" rel="self" type="application/atom+xml"/><link href="https://goncalor00.github.io/phd/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-09T16:27:33+00:00</updated><id>https://goncalor00.github.io/phd/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">First steps on 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/calibration_3d/" rel="alternate" type="text/html" title="First steps on 3D reconstruction"/><published>2024-05-24T16:40:16+00:00</published><updated>2024-05-24T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/calibration_3d</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/calibration_3d/"><![CDATA[<p>I started by trying to build an entire pipeline to see the end results of the 3D points. This way it is easier to see if something is going wrong in one of the steps.</p> <p>I tried to retrieve the intrinsic parameters of my smartphone’s camera from the image’s metadata, but I was getting weird results. Then I tried to get them manually, but someone in the lab reminded me that smartphones have auto-focus and the parameters change. To avoid messing with my phone, I simply used another camera that was available and not being used by anyone in the lab (an Orbbec Astra Pro) and got the intrinsic parameters using a chessboard and a ROS tool</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240424_162653.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I used SIFT once more to get the matching points from two viewpoints:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Linked%20Points-480.webp 480w,/phd/assets/posts/img/05_2024/Linked%20Points-800.webp 800w,/phd/assets/posts/img/05_2024/Linked%20Points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Linked%20Points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Then, found the Essential matrix → Rotation and Translation → Projection matrices</p> <p>With the matching points and projection matrices, I triangulated and plotted the points (ignoring the distortion parameters) and got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/3D%20plots.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>But the Essential matrix is defined only up to scale (in the illustration it is the fundamental matrix, but the essential matrix comes from the fundamental matrix knowing the intrinsic parameters).</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Source: <a href="https://www.youtube.com/watch?v=izpYAwJ0Hlw&amp;list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo&amp;index=10">https://www.youtube.com/watch?v=izpYAwJ0Hlw&amp;list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo&amp;index=10</a> </div> <p>OpenCV has a function that solves this by setting the distance between cameras to 1, which means that the distance between points is relative to the distance between cameras. We can retrieve the real scale factor by having an object in the image of known dimensions.</p> <p>Using the distortion factors, I got weird results and I haven’t found out yet what I did wrong:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/3DPlotdistort.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Re-calibrating the camera with more images and still ignoring the distortion parameters, the results are apparently better</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/teste.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Then I experimented with bending the paper like this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/img0_perpendicular-480.webp 480w,/phd/assets/posts/img/05_2024/img0_perpendicular-800.webp 800w,/phd/assets/posts/img/05_2024/img0_perpendicular-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/img0_perpendicular.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>With SIFT I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/linked_pointsv2-480.webp 480w,/phd/assets/posts/img/05_2024/linked_pointsv2-800.webp 800w,/phd/assets/posts/img/05_2024/linked_pointsv2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/linked_pointsv2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>And plotting, I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/bend.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>This result gave me more confidence about the reliability of the previous steps.</p> <p>Then, I started working on a 3D reconstruction of the object. I started by trying some OpenCV functions. The functions that I found create disparity maps, and the goal was to create a 3D plot from those maps. With this approach, I only got bad results without any meaning, and I gave up.</p> <p>Using the two images from the two viewpoints and doing the dot product between the fundamental matrix and the coordinates of the pixel in analyses in the first image, the result is a vector containing the $a$, $b$, and $c$ parameters of the epipolar line ($ax + by + c = 0$) in the second image:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2215-44-12-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Inside the epipolar line will be the point of image 2 that matches the point in analyses of image 1. This reduces the search of the matching point by a lot, being a 1D search instead of a 2D search.</p> <p>Although the search is less than without the epipolar line, there are many points, and even using the simplest algorithm possible to compare the windows of pixels (absolute distance between arrays—cv2.norm()) from the two images, the time of execution using Python is more than 10 minutes.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2314-59-48-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>The cv2.norm() is not ideal for this use case and generates a lot of errors, but by removing isolated points, I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/point_cloud.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>After that, I decided to try to optimize the code, but I only got minor improvements. So, I took a C++ course and implemented the slowest function in C++. The major part of the program is written in Python, and the function with high computation cost is written in C++ and compiled. With this, the computation time of the function is less than 30 seconds, more than 30x faster than Python.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240522_151420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240523_191350.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The current pipeline looks like this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/pointcloud_explained.svg" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/pointcloud_explained.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Unanswered questions:</p> <ul> <li>What are the best algorithms (green boxes) for the current pipeline?</li> <li>How can I evaluate the quality of the results?</li> <li>How can I estimate the intrinsic parameters after changing the focal length?</li> <li>In a stream of images (after motion scope), should I rerun the current pipeline each time, or should I use some kind of tracking algorithm?</li> </ul>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><category term="Calibration"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Feature Matching</title><link href="https://goncalor00.github.io/phd/reports/2024/feature_matching/" rel="alternate" type="text/html" title="Feature Matching"/><published>2024-04-18T16:40:16+00:00</published><updated>2024-04-18T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/feature_matching</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/feature_matching/"><![CDATA[<p>This week, I defined the problem and the objectives. In addition to that, I did the planning for the first stage of the project (see <a href="/phd">Goals</a> and <a href="/phd/timeline">Tasks</a>).</p> <p>In terms of development itself, I tried something that I had studied before in SAVI (UA course): The SIFT algorithm (details: <a href="/phd/courses/computer_vision/feature_matching/sift">Feature recognition and matching</a>). Using this algorithm, we get an array of features for n objects in the image. These features are based on the object’s appearance at particular interest points and are invariant to image scale and rotation. They are also robust to changes in illumination, noise, and <strong>minor changes in viewpoint</strong>.</p> <p>Using SIFT in two images, we get two arrays of features. These features are essentially vectors, and we can compute the distance between vectors from the two images. The correspondent objects between images have a small distance between feature vectors. The following image illustrates the results. Detected objects with unique features are marked with a circle, and the corresponding objects are connected with a line.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 1000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p> <p>However, there is another problem with this… Most of the time, more than one object is very similar (thus, having a small distance between feature vectors) to the corresponding one. The following image shows this problem—there are incorrect correspondences between objects, but the objects are similar.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example4(2)-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example4(2)-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example4(2)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example4(2).jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 1000 and without David Lowe's filtration.</em> </div> </div> <p><br/></p> <p>There are various techniques to mitigate this issue. One that works very well is using the Lowe’s ratio, which is the ratio between the best and second best correspondences for an object. Using this, we can reject the most ambiguous correspondences and keep the robust ones. Using this, most of the correspondences between objects from the previous image were rejected:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example2-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example2-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 1000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p> <p>Increasing the number of objects gives us a good amount of valid correspondence.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example3-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example3-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 5000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p> <p>However, SIFT has a big limitation, which is the change of viewpoint:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example4(1)(4)-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example4(1)(4)-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example4(1)(4)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example4(1)(4).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 40000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Using MotionScope to see the problem</title><link href="https://goncalor00.github.io/phd/reports/2024/trying_motionscope/" rel="alternate" type="text/html" title="Using MotionScope to see the problem"/><published>2024-04-11T16:40:16+00:00</published><updated>2024-04-11T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/trying_motionscope</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/trying_motionscope/"><![CDATA[<h1 id="meeting-report">Meeting report</h1> <p>In this meeting, we used the MotionScope to see the problem: It is not possible to accurately see the mode of vibration. It is easy to get an optical illusion when we expect a certain mode of vibration, but in reality, it is another mode (For example, torsion and flexion).</p> <p>An idea to solve this is to use the camera in two places and identify common points. Then get the 3D motion of the pixels and, for example, plot them in a way that is possible to change the point of view. It would also be interesting to wrap the points around a surface (like an STL) to get a better view.</p> <p>Then we discussed a bit about what we are planning to do to solve our problems.</p> <h1 id="next-meeting-11042024-1400">Next meeting (11/04/2024 14:00)</h1> <p>Title and objectives; Planning</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Problem"/><category term="MotionScope"/><summary type="html"><![CDATA[Meeting report]]></summary></entry><entry><title type="html">Towards defining the problem</title><link href="https://goncalor00.github.io/phd/reports/2024/defining_problem/" rel="alternate" type="text/html" title="Towards defining the problem"/><published>2024-04-05T16:40:16+00:00</published><updated>2024-04-05T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/defining_problem</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/defining_problem/"><![CDATA[<p>In this meeting, we talked about my problem and João’s problem.</p> <p>In my case, the MotionScope can’t capture movements perpendicular to the camera’s plane. This can also cause problems since this motion can be perceived as movement in other directions. In addition, the motion is calculated only in the camera plane, in pixels.</p> <p>The idea is to capture more than one viewpoint, identify the points in each viewpoint, and associate correspondent points across all views. Knowing the correspondence of points, it may also be possible to compute the relative position of each camera (might need more references - study these possibilities). After computing both extrinsic and intrinsic parameters for each camera, the idea is to get the 3D movement of each point.</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Problem"/><summary type="html"><![CDATA[Meeting report]]></summary></entry><entry><title type="html">ATOM experiments</title><link href="https://goncalor00.github.io/phd/reports/2024/atom_experiments/" rel="alternate" type="text/html" title="ATOM experiments"/><published>2024-04-03T16:40:16+00:00</published><updated>2024-04-03T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/atom_experiments</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/atom_experiments/"><![CDATA[<p>I started the week by trying to clarify the roots of the problem that I’m trying to solve with Professor Rui Moreira. The issue is that MotionScope licenses aren’t available before Easter, so I’ll have to wait to explore the software and understand the problem. Meanwhile, I continued watching some online courses related to the basics of mechanical vibrations and machine learning and exploring the calibration of cameras.</p> <p>For camera calibration, I thought of finding a way to get the “ground truth” first to evaluate the quality of the camera calibration system that I will implement or develop. For that, I’ve been exploring <a href="https://github.com/lardemua/atom"><strong>ATOM</strong></a>, a set of calibration tools for robotic applications developed and maintained by teachers, researchers, and students of the University of Aveiro. This might be an overkill solution, but it is safer to use a tool that is well-understood by the LAR community. Thus, getting support is easier than using other available solutions or developing something from scratch.</p> <p>Since ATOM is a ROS-based application, it runs on Ubuntu 20.04, which is beginning to become outdated. I used Docker to install ATOM to avoid using an old Linux distro directly on my computer and to prevent future problems. This allows the use of ATOM in other operating systems, even Windows.</p> <p>The <a href="https://lardemua.github.io/atom_documentation/">ATOM documentation</a> provides examples (<a href="https://lardemua.github.io/atom_documentation/examples/">A</a>, <a href="https://github.com/lardemua/atom/tree/noetic-devel/atom_examples">B</a>), one of which is the use of <a href="https://github.com/lardemua/atom/tree/noetic-devel/atom_examples/rrbot">two RGB cameras</a>. There are also RGB-D examples that might be useful for João.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/Screenshot_20240402_140350-480.webp 480w,/phd/assets/posts/img/04_2024/Screenshot_20240402_140350-800.webp 800w,/phd/assets/posts/img/04_2024/Screenshot_20240402_140350-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/Screenshot_20240402_140350.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/Screenshot_20240402_140637-480.webp 480w,/phd/assets/posts/img/04_2024/Screenshot_20240402_140637-800.webp 800w,/phd/assets/posts/img/04_2024/Screenshot_20240402_140637-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/Screenshot_20240402_140637.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the <a href="https://doi.org/10.1016/j.eswa.2022.118000">ATOM paper</a>, some works related to the calibration of RGB cameras are mentioned. Some of these works are related to calibration without checkerboards (in the case of ATOM would be useful for online calibration) but “Patternless calibration approaches have the advantage of operating continuously if necessary, but loose in accuracy and robustness when compared to offline, one shot procedures. As such, offline calibrations are still the most commonly used.”. I will read those works to see how these calibrations are done.</p> <p>For the calibration of RGB cameras, ATOM detects the points using a pattern in both cameras—this is done for a set of “collections” (data from all sensors at the same time). Then, with a user guess for an approximated position and orientation of the cameras, it reprojects the pattern points. Then, it starts an optimization process to minimize the error between the points of the pattern and the reprojected points.</p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="ATOM"/><category term="ROS"/><category term="Simulation"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">First meeting</title><link href="https://goncalor00.github.io/phd/reports/2024/first-meeting/" rel="alternate" type="text/html" title="First meeting"/><published>2024-03-20T16:40:16+00:00</published><updated>2024-03-20T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/first-meeting</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/first-meeting/"><![CDATA[<h1 id="support-material">Support material</h1> <p><a href="/phd/">Goals</a> and <a href="/phd/timeline/">Tasks</a></p> <h1 id="meeting-report">Meeting report</h1> <p>I need to define better the problem description:</p> <ul> <li>Learn how the MotionScope works</li> <li>What is possible to do with the MotionScope, and what do we want to do that isn’t currently possible?</li> </ul> <p>After a better understanding, I should talk again with prof. Rui Moreira, in order to adjust the “final” problem description and planning</p> <p>Next LAR Meeting: 11/04/2024 (5, 10, or 15 minutes???)</p> <h1 id="next-meeting-03042024-1030">Next meeting (03/04/2024 10:30)</h1> <p>Finnish the problem description and planification</p> <p><a href="/phd/">Goals</a> and <a href="/phd/timeline/">Tasks</a></p>]]></content><author><name></name></author><category term="meeting-report"/><category term="MotionScope"/><category term="Research"/><summary type="html"><![CDATA[Meeting report]]></summary></entry></feed>