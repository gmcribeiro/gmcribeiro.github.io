<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://goncalor00.github.io/phd/feed.xml" rel="self" type="application/atom+xml"/><link href="https://goncalor00.github.io/phd/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-09T20:34:56+00:00</updated><id>https://goncalor00.github.io/phd/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">LightGlue on 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/3d_lightglue/" rel="alternate" type="text/html" title="LightGlue on 3D reconstruction"/><published>2024-10-25T16:40:16+00:00</published><updated>2024-10-25T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/3d_lightglue</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/3d_lightglue/"><![CDATA[<p>The current “final” goal can be divided into two parts:</p> <p>“<span style="color:DodgerBlue">Identification techniques of operational vibration modes with MotionScope</span> <span style="color:MediumSeaGreen; font-weight:bold;">assisted by stereoscopy</span>”.</p> <p>From the beginning, I’ve been focusing on the “<span style="color:MediumSeaGreen; font-weight:bold;">assisted by stereoscopy</span>” to create a 3D representation from 2D images. The 3D representation is essential for easier visualization of the magnified motion of the objects.</p> <p>Initial question: <span style="color:MediumSeaGreen; font-weight:bold;">How can we create a 3D representation from 2D images?</span></p> <p>1 - Stereo-based techniques (including multi-view)</p> <ul> <li>Accurately calibrated cameras</li> <li>Dense feature matching across images captured from slightly different viewing angles</li> <li>Triangulation to recover the 3D coordinates of the image pixels.</li> </ul> <p>2- Structure from Motion</p> <ul> <li>This recovers 3D structure by analyzing the motion of the camera and the changes in the images - The only way that I see that this could work is as an auxiliary solution to get more information about the object in the study and about the new camera position and parameters while moving from viewpoint A to viewpoint B.</li> </ul> <p>3 - Shape-from-Silhouette</p> <ul> <li>It reconstructs the 3D shape using the silhouettes of objects from multiple views. This solution was first published in 1994 and is generally used to generate rough, undetailed 3D models quickly. I could try it to see how good the results are and check if this technique is helpful for this application, as it is also used in the loss function of some deep-learning models</li> </ul> <p>4 - Photometric Stereo</p> <ul> <li>Uses variations in shading to infer the 3D shape of objects - I don’t think this is a good path to follow since motion scope relies on the quality of the light source, conflicting with this technique.</li> </ul> <p>5 - Learning-based</p> <ul> <li>There are multiple approaches with learning-based techniques, such as: <ul> <li>3D reconstruction problem as a recognition problem</li> <li>Multi-tasked neural networks - This allows for better generalization of feature recognition</li> <li>End-to-end 3D reconstruction - eliminating the need to design multiple handcrafted stages</li> <li>Single and multi-image 3D reconstruction</li> <li>Depth estimation</li> <li>Disparity maps</li> <li>Feature matching</li> </ul> </li> <li>Some don’t require intrinsic or extrinsic parameters</li> </ul> <p><span style="color:MediumSeaGreen; font-weight:bold;">How can we evaluate 3D representations and compare different methods?</span></p> <p><span style="color:MediumSeaGreen; font-weight:bold;">Are there good enough solutions?</span></p> <p><span style="color:MediumSeaGreen; font-weight:bold;">Can I combine some techniques to improve the results for our use case?</span></p> <p>Until now, what I have done is a 3D reconstruction using uncalibrated stereo, which is similar to <strong>Stereo-based techniques</strong> but with an additional step of “calibration” in the beginning:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/Untitled-Frame-3.svg" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/Untitled-Frame-3.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>In the previous meeting, I showed a promising algorithm for feature-matching, the LightGlue. My idea was to use this algorithm (or something similar) for the “Camera calibration”.</p> <p>A small change in viewpoint: Results with SIFT:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_easy_matching-480.webp 480w,/phd/assets/posts/img/10_2024/sift_easy_matching-800.webp 800w,/phd/assets/posts/img/10_2024/sift_easy_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_easy_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot-480.webp 480w,/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot-800.webp 800w,/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/2024-10-24-182342_hyprshot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Results with LightGlue:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_easy_matching-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_easy_matching-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_easy_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_easy_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_easy-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_easy-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_easy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_easy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>A more significant change in viewpoint:</p> <p>Results with SIFT:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_hard_matching-480.webp 480w,/phd/assets/posts/img/10_2024/sift_hard_matching-800.webp 800w,/phd/assets/posts/img/10_2024/sift_hard_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_hard_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_hard-480.webp 480w,/phd/assets/posts/img/10_2024/sift_hard-800.webp 800w,/phd/assets/posts/img/10_2024/sift_hard-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_hard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Results with LightGlue:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hard_matching-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hard_matching-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hard_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hard_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hard-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hard-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hard-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Finally, with a big viewpoint change:</p> <p>SIFT:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/sift_hardest_matching-480.webp 480w,/phd/assets/posts/img/10_2024/sift_hardest_matching-800.webp 800w,/phd/assets/posts/img/10_2024/sift_hardest_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/sift_hardest_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>There are not enough matching points to compute camera poses and 3D reconstruction</p> <p>LightGlue:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hardest_matching-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hardest_matching-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hardest_matching-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hardest_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Something went wrong with the camera poses with lightglue, but I haven’t figured out the issue yet.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/lightglue_hardest-480.webp 480w,/phd/assets/posts/img/10_2024/lightglue_hardest-800.webp 800w,/phd/assets/posts/img/10_2024/lightglue_hardest-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/lightglue_hardest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Since this new kind of feature-matching algorithm can find so many matches, I wonder if I could use this matching for the “dense matching” part… It would need more viewpoints but would be much lighter computationally. I could also use the epipolar lines to validate the results of the feature-matching…</p> <p>Although there seem to be many feature matches, there are very few for a 3D representation. Dense matching is still needed.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot-480.webp 480w,/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot-800.webp 800w,/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/2024-10-25-105955_hyprshot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Métricas</p> <p>Criar outliers artificiais para robustez</p> <p>automatizar a definição da dificuldade</p> <p>Gazebo</p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Testing LightGlue</title><link href="https://goncalor00.github.io/phd/reports/2024/testing_lightglue/" rel="alternate" type="text/html" title="Testing LightGlue"/><published>2024-10-11T16:40:16+00:00</published><updated>2024-10-11T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/testing_lightglue</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/testing_lightglue/"><![CDATA[<p>In the last meeting, we concluded that I should research more state-of-the-art solutions that solve entirely or partially the problems of camera pose retrieval and 3D reconstruction. Another idea has been to explore ways to use learning to find the homography, camera position, etc. It turns out that it is all related, and it is what people have been doing.</p> <p>So far, I have only explored feature matching, and, to be honest, I don’t have the knowledge (yet) of machine learning to understand how these algorithms were made. I started a machine learning course, and I did 1/4 of the course. I paused that course because I realized that my knowledge of probabilities wasn’t near enough to understand the machine learning concepts. So, I’m now doing a course about probabilities for machine learning.</p> <p>I tried one detector-based algorithm - LightGlue - for feature match on very challenging scenarios (drastic viewpoint changes), but the results speak for themselves:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/LightGlue_feature_match-480.webp 480w,/phd/assets/posts/img/10_2024/LightGlue_feature_match-800.webp 800w,/phd/assets/posts/img/10_2024/LightGlue_feature_match-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/LightGlue_feature_match.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/LightGlue_feature_match_2-480.webp 480w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_2-800.webp 800w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/LightGlue_feature_match_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/10_2024/LightGlue_feature_match_3-480.webp 480w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_3-800.webp 800w,/phd/assets/posts/img/10_2024/LightGlue_feature_match_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/10_2024/LightGlue_feature_match_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>There are still some errors, but this is a massive improvement compared to what I used.</p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Paper sheet holder</title><link href="https://goncalor00.github.io/phd/reports/2024/paper_sheet_holder/" rel="alternate" type="text/html" title="Paper sheet holder"/><published>2024-07-16T16:40:16+00:00</published><updated>2024-07-16T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/paper_sheet_holder</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/paper_sheet_holder/"><![CDATA[<p>During this time, I didn’t advance much in the work itself, as I spent the time learning more about the basics of computer vision. Meanwhile, I built a support for the sheet paper:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/07_2024/1000010432-480.webp 480w,/phd/assets/posts/img/07_2024/1000010432-800.webp 800w,/phd/assets/posts/img/07_2024/1000010432-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/07_2024/1000010432.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/07_2024/1000010431-480.webp 480w,/phd/assets/posts/img/07_2024/1000010431-800.webp 800w,/phd/assets/posts/img/07_2024/1000010431-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/07_2024/1000010431.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/07_2024/1000010430-480.webp 480w,/phd/assets/posts/img/07_2024/1000010430-800.webp 800w,/phd/assets/posts/img/07_2024/1000010430-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/07_2024/1000010430.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="meeting-support"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting support]]></summary></entry><entry><title type="html">Improving 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/improving_3d_report/" rel="alternate" type="text/html" title="Improving 3D reconstruction"/><published>2024-06-21T17:40:16+00:00</published><updated>2024-06-21T17:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/improving_3d_report</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/improving_3d_report/"><![CDATA[<h1 id="meeting-report">Meeting report</h1> <p>In this meeting, I showed my ideas for improving 3D reconstruction.</p> <h1 id="next-meeting-16072024-">Next meeting (16/07/2024 :??)</h1> <p>Explore solutions from other people and explore more papers</p> <p>Start exploring neural networks</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting report]]></summary></entry><entry><title type="html">Improving 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/improving_3d/" rel="alternate" type="text/html" title="Improving 3D reconstruction"/><published>2024-06-21T16:40:16+00:00</published><updated>2024-06-21T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/improving_3d</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/improving_3d/"><![CDATA[<p>The idea of the previous meeting was to use more viewpoints, but the current “solution” is not robust enough. So, I worked more on studying ways to improve the robustness of each pipeline step.</p> <h3 id="quick-concept-recap">Quick concept recap</h3> <p>Knowing the fundamental matrix (obtained by knowing at least 8 matching points between 2 images), we can map a point of the first image to a line of the second image. The correspondent point, if present in the image, will be somewhere in that line.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2314-59-48-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="caption"> This narrows the search to a 1D search, simplifying the problem considerably. Doing this to the entire image allows, with the intrinsic parameters and more processing, the creation of a 3D cloud. This process is called (not entirely sure) dense matching. </div> <p>I started by trying to improve the “dense matching” algorithm since the matching algorithm in the 1D search is very simplistic and not robust enough. So, I started adapting the code to be compatible with feature-detection algorithms, which I believe will give much better results but take much more time to compute.</p> <p>This leads to two options:</p> <ul> <li> Send the ROI of the image for the 1D search: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1)-480.webp 480w,/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1)-800.webp 800w,/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/Untitled%20-%20Frame%201(1).jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> I could not figure out if the feature-detection algorithms ignore the black region or not, and that's the reason why there is an option 2 </li> <li> The second option is more tricky. It involves sending a rectangular image to the feature-detection algorithm to ensure that only the ROI is analyzed. To achieve that, the first step is to rotate the image with the angle of inclination of the epipolar line resulting on the image of the window “Rotated image”; This way, the ROI is a rectangular section of the image, like in the window “Cropped to the window height”, removing the black regions. <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy)-480.webp 480w,/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy)-800.webp 800w,/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/Screenshot_20240604_174243%20(copy).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> I have some doubts about this method, starting with the fact that the epipolar line is not well defined (the thickness of the line is not 1 pixel—window “Section of the cropped image”). Worse than that, I don’t know if applying the feature-detection algorithm to the entire image with a very high number of keypoints and then applying the ROI only to the feature-match algorithm won't give similar results with less computational cost. </li> </ul> <p>Eventually, I put these studies on hold and studied ways to find the common region between the two viewpoints. This will help in the future to solve the occlusions problem. ChatGPT and Bing chat gave me the following directions:</p> <ul> <li> “Traditional” approaches: <ul> <li> Estimate homography using feature detection and matching algorithms and use the homography to warp one image onto the other and find common regions. Here is an illustration of warping operation:<br/> Image 1: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/img_0_1-480.webp 480w,/phd/assets/posts/img/06_2024/img_0_1-800.webp 800w,/phd/assets/posts/img/06_2024/img_0_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/img_0_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> Image 2: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/img_0_2-480.webp 480w,/phd/assets/posts/img/06_2024/img_0_2-800.webp 800w,/phd/assets/posts/img/06_2024/img_0_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/img_0_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> Aligned image 1: <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024-480.webp 480w,/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024-800.webp 800w,/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/aligned%20image%201_screenshot_20.06.2024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> The chat only suggests doing a bit-wise operation to find the common region, but that doesn’t make sense since the background is totally different. I kept this here because the object (a sheet of paper) gets a similar shape between the two images, and I could use that to identify and isolate the object. </li> <li> Block Comparison: Basically, divide the image into small blocks and compare each block of image 1 to every block of image 2 and save the blocks with similarity bigger than a threshold </li> </ul> </li> <li> Machine learning: From the two chats, the suggestions were: Siamese networks, collaborative segmentation, stereo matching networks, image registration networks, and deep image matching. I didn’t explore these fields because I felt that I needed a deeper understanding of machine learning, and I started taking an online course on the subject. </li> </ul> <p>During these studies, I had a problem with my computer: I neglected the updates and started not being able to install anything. I formatted the computer and took the opportunity to start working only on docker environments to prevent losing the dependencies of the code that I have in the event of having to format the computer again. When I tested the code, it was much slower in the C++ part. After building from source different versions of OpenCV, GCC an G++ I finally found the issue 🤦‍♂️:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot-480.webp 480w,/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot-800.webp 800w,/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/06_2024/2024-06-20-164316_hyprshot.png" class="img-fluid rounded z-depth-1" width="250" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">First steps on 3D reconstruction</title><link href="https://goncalor00.github.io/phd/reports/2024/calibration_3d/" rel="alternate" type="text/html" title="First steps on 3D reconstruction"/><published>2024-05-24T16:40:16+00:00</published><updated>2024-05-24T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/calibration_3d</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/calibration_3d/"><![CDATA[<p>I started by trying to build an entire pipeline to see the end results of the 3D points. This way it is easier to see if something is going wrong in one of the steps.</p> <p>I tried to retrieve the intrinsic parameters of my smartphone’s camera from the image’s metadata, but I was getting weird results. Then I tried to get them manually, but someone in the lab reminded me that smartphones have auto-focus and the parameters change. To avoid messing with my phone, I simply used another camera that was available and not being used by anyone in the lab (an Orbbec Astra Pro) and got the intrinsic parameters using a chessboard and a ROS tool</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240424_162653-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240424_162653.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I used SIFT once more to get the matching points from two viewpoints:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Linked%20Points-480.webp 480w,/phd/assets/posts/img/05_2024/Linked%20Points-800.webp 800w,/phd/assets/posts/img/05_2024/Linked%20Points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Linked%20Points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Then, found the Essential matrix → Rotation and Translation → Projection matrices</p> <p>With the matching points and projection matrices, I triangulated and plotted the points (ignoring the distortion parameters) and got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/3D%20plots.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>But the Essential matrix is defined only up to scale (in the illustration it is the fundamental matrix, but the essential matrix comes from the fundamental matrix knowing the intrinsic parameters).</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240425_154214(1).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Source: <a href="https://www.youtube.com/watch?v=izpYAwJ0Hlw&amp;list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo&amp;index=10">https://www.youtube.com/watch?v=izpYAwJ0Hlw&amp;list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo&amp;index=10</a> </div> <p>OpenCV has a function that solves this by setting the distance between cameras to 1, which means that the distance between points is relative to the distance between cameras. We can retrieve the real scale factor by having an object in the image of known dimensions.</p> <p>Using the distortion factors, I got weird results and I haven’t found out yet what I did wrong:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/3DPlotdistort.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Re-calibrating the camera with more images and still ignoring the distortion parameters, the results are apparently better</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/teste.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Then I experimented with bending the paper like this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/img0_perpendicular-480.webp 480w,/phd/assets/posts/img/05_2024/img0_perpendicular-800.webp 800w,/phd/assets/posts/img/05_2024/img0_perpendicular-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/img0_perpendicular.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>With SIFT I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/linked_pointsv2-480.webp 480w,/phd/assets/posts/img/05_2024/linked_pointsv2-800.webp 800w,/phd/assets/posts/img/05_2024/linked_pointsv2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/linked_pointsv2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>And plotting, I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/bend.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>This result gave me more confidence about the reliability of the previous steps.</p> <p>Then, I started working on a 3D reconstruction of the object. I started by trying some OpenCV functions. The functions that I found create disparity maps, and the goal was to create a 3D plot from those maps. With this approach, I only got bad results without any meaning, and I gave up.</p> <p>Using the two images from the two viewpoints and doing the dot product between the fundamental matrix and the coordinates of the pixel in analyses in the first image, the result is a vector containing the $a$, $b$, and $c$ parameters of the epipolar line ($ax + by + c = 0$) in the second image:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2215-44-12-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>Inside the epipolar line will be the point of image 2 that matches the point in analyses of image 1. This reduces the search of the matching point by a lot, being a 1D search instead of a 2D search.</p> <p>Although the search is less than without the epipolar line, there are many points, and even using the simplest algorithm possible to compare the windows of pixels (absolute distance between arrays—cv2.norm()) from the two images, the time of execution using Python is more than 10 minutes.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/2024-05-2314-59-48-ezgif.com-video-to-mp4-converter.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>The cv2.norm() is not ideal for this use case and generates a lot of errors, but by removing isolated points, I got this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <video src="/phd/assets/posts/vid/05_2024/point_cloud.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>After that, I decided to try to optimize the code, but I only got minor improvements. So, I took a C++ course and implemented the slowest function in C++. The major part of the program is written in Python, and the function with high computation cost is written in C++ and compiled. With this, the computation time of the function is less than 30 seconds, more than 30x faster than Python.</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240522_151420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240522_151420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-480.webp 480w,/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-800.webp 800w,/phd/assets/posts/img/05_2024/Screenshot_20240523_191350-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/Screenshot_20240523_191350.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The current pipeline looks like this:</p> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/05_2024/pointcloud_explained.svg" sizes="95vw"/> <img src="/phd/assets/posts/img/05_2024/pointcloud_explained.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Unanswered questions:</p> <ul> <li>What are the best algorithms (green boxes) for the current pipeline?</li> <li>How can I evaluate the quality of the results?</li> <li>How can I estimate the intrinsic parameters after changing the focal length?</li> <li>In a stream of images (after motion scope), should I rerun the current pipeline each time, or should I use some kind of tracking algorithm?</li> </ul>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><category term="3D Reconstruction"/><category term="Calibration"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Using a vibrometer</title><link href="https://goncalor00.github.io/phd/reports/2024/vibrometer/" rel="alternate" type="text/html" title="Using a vibrometer"/><published>2024-05-24T16:40:16+00:00</published><updated>2024-05-24T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/vibrometer</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/vibrometer/"><![CDATA[<h1 id="meeting-report">Meeting report</h1> <p>In this meeting, we used the vibrometer to measure the vibrations from multiple directions.</p> <h1 id="next-meeting-21062024-1430">Next meeting (21/06/2024 14:30)</h1> <p>3D reconstruction using more than two viewpoints (and camera poses recovery);</p> <p>Occlusions problem;</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Vibrometer"/><summary type="html"><![CDATA[Meeting report]]></summary></entry><entry><title type="html">Feature Matching</title><link href="https://goncalor00.github.io/phd/reports/2024/feature_matching/" rel="alternate" type="text/html" title="Feature Matching"/><published>2024-04-18T16:40:16+00:00</published><updated>2024-04-18T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/feature_matching</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/feature_matching/"><![CDATA[<p>This week, I defined the problem and the objectives. In addition to that, I did the planning for the first stage of the project (see <a href="/phd">Goals</a> and <a href="/phd/timeline">Tasks</a>).</p> <p>In terms of development itself, I tried something that I had studied before in SAVI (UA course): The SIFT algorithm (details: <a href="/phd/courses/computer_vision/feature_matching/sift">Feature recognition and matching</a>). Using this algorithm, we get an array of features for n objects in the image. These features are based on the object’s appearance at particular interest points and are invariant to image scale and rotation. They are also robust to changes in illumination, noise, and <strong>minor changes in viewpoint</strong>.</p> <p>Using SIFT in two images, we get two arrays of features. These features are essentially vectors, and we can compute the distance between vectors from the two images. The correspondent objects between images have a small distance between feature vectors. The following image illustrates the results. Detected objects with unique features are marked with a circle, and the corresponding objects are connected with a line.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 1000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p> <p>However, there is another problem with this… Most of the time, more than one object is very similar (thus, having a small distance between feature vectors) to the corresponding one. The following image shows this problem—there are incorrect correspondences between objects, but the objects are similar.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example4(2)-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example4(2)-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example4(2)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example4(2).jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 1000 and without David Lowe's filtration.</em> </div> </div> <p><br/></p> <p>There are various techniques to mitigate this issue. One that works very well is using the Lowe’s ratio, which is the ratio between the best and second best correspondences for an object. Using this, we can reject the most ambiguous correspondences and keep the robust ones. Using this, most of the correspondences between objects from the previous image were rejected:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example2-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example2-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 1000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p> <p>Increasing the number of objects gives us a good amount of valid correspondence.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example3-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example3-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 5000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p> <p>However, SIFT has a big limitation, which is the change of viewpoint:</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/phd/assets/posts/img/04_2024/matches_example4(1)(4)-480.webp 480w,/phd/assets/posts/img/04_2024/matches_example4(1)(4)-800.webp 800w,/phd/assets/posts/img/04_2024/matches_example4(1)(4)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/phd/assets/posts/img/04_2024/matches_example4(1)(4).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Sift features = 40000 and David Lowe’s ratio = 30%</em> </div> </div> <p><br/></p>]]></content><author><name></name></author><category term="meeting-support"/><category term="meeting-preparation"/><category term="Computer Vision"/><category term="Feature Matching"/><summary type="html"><![CDATA[Meeting preparation and support]]></summary></entry><entry><title type="html">Using MotionScope to see the problem</title><link href="https://goncalor00.github.io/phd/reports/2024/trying_motionscope/" rel="alternate" type="text/html" title="Using MotionScope to see the problem"/><published>2024-04-11T16:40:16+00:00</published><updated>2024-04-11T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/trying_motionscope</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/trying_motionscope/"><![CDATA[<h1 id="meeting-report">Meeting report</h1> <p>In this meeting, we used the MotionScope to see the problem: It is not possible to accurately see the mode of vibration. It is easy to get an optical illusion when we expect a certain mode of vibration, but in reality, it is another mode (For example, torsion and flexion).</p> <p>An idea to solve this is to use the camera in two places and identify common points. Then get the 3D motion of the pixels and, for example, plot them in a way that is possible to change the point of view. It would also be interesting to wrap the points around a surface (like an STL) to get a better view.</p> <p>Then we discussed a bit about what we are planning to do to solve our problems.</p> <h1 id="next-meeting-11042024-1400">Next meeting (11/04/2024 14:00)</h1> <p>Title and objectives; Planning</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Problem"/><category term="MotionScope"/><summary type="html"><![CDATA[Meeting report]]></summary></entry><entry><title type="html">Towards defining the problem</title><link href="https://goncalor00.github.io/phd/reports/2024/defining_problem/" rel="alternate" type="text/html" title="Towards defining the problem"/><published>2024-04-05T16:40:16+00:00</published><updated>2024-04-05T16:40:16+00:00</updated><id>https://goncalor00.github.io/phd/reports/2024/defining_problem</id><content type="html" xml:base="https://goncalor00.github.io/phd/reports/2024/defining_problem/"><![CDATA[<p>In this meeting, we talked about my problem and João’s problem.</p> <p>In my case, the MotionScope can’t capture movements perpendicular to the camera’s plane. This can also cause problems since this motion can be perceived as movement in other directions. In addition, the motion is calculated only in the camera plane, in pixels.</p> <p>The idea is to capture more than one viewpoint, identify the points in each viewpoint, and associate correspondent points across all views. Knowing the correspondence of points, it may also be possible to compute the relative position of each camera (might need more references - study these possibilities). After computing both extrinsic and intrinsic parameters for each camera, the idea is to get the 3D movement of each point.</p>]]></content><author><name></name></author><category term="meeting-report"/><category term="Problem"/><summary type="html"><![CDATA[Meeting report]]></summary></entry></feed>